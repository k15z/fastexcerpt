{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FastExcerpt","text":"<p>Fast extraction of relevant excerpts for long-form text.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>The motivation for this project was the need for a lightning fast library for extracting meaningful excerpts from long-form text. This library is used by AO3 Disco, a  fanfiction recommendation service which uses this library to extract excerpts for further analysis.</p> <p>As an example, consider predicting whether a novel contains \"time travel\" as a plot device:</p> <ul> <li>If you randomly select a few excerpts to analyze, it's quite possible you'll miss the relevant     passage in the text.</li> <li>If you pass the entire work through a classic model (BoW, etc.), it's possible that you'll be     able to classify it, but the precision/recall might not be great.</li> <li>If you pass the entire work through a deep learning model (transformers, etc.), it might be     able to achieve good results if it's given enough data, but would be too slow for many types     of applications.</li> </ul> <p>A much better alternative would be to add some handcrafted keywords - i.e. search for \"time travel\" as keywords and look at just those excerpts. However, this approach isn't scalable if there are  tens of thousands of tags like \"time travel\".</p> <p>The goal of <code>FastExcerpt</code> is to combine the strengths of these approaches by using fast models to efficiently extract relevant excerpts for further analysis.</p>"},{"location":"#usage","title":"Usage","text":"<p>Construct a FastExcerpt object:</p> <pre><code>fe = FastExcerpt(\n    window_size=3, # Look for 3-sentence long excerpts.\n    max_hash_size=10000, # Hash size used for the ranking model.\n)\n\n# Extract 1 excerpt which is 3 sentences long.\nexcerpts = fe.excerpts(\"... a long document ...\", k=1)\n</code></pre> <p>The standard use case is when you have a downstream binary classification task. You can simply pass your data in as two lists:</p> <pre><code>docs = [\n    \"This is a long document. It can be over 100,000 words long!\",\n    ... # more documents\n]\nlabels = [\n    1,\n    ...\n]\nfe.fit(docs, labels)\n</code></pre> <p>In this case, the model will learn to select excerpts that are maximize the performance of a  simple classifier on the specified classification task.</p> <p>Consider an example where you are predicting whether the work should be rated \"Explicit\" - in  this case, the model should learn to select excerpts of text that contain explicit language.</p>"},{"location":"benchmark/","title":"Benchmark","text":"<p>On this page, we summarize our results from applying FastExcerpt to an internal dataset of works  on Archive of Our Own consisting of ~150,000 works that are tagged with various labels by the  authors of the works. The lengths of the works range from 1,000 words to over 500,000 words and  we test different methods of extracting 5-sentence excerpts for predicting the tags.</p>"},{"location":"benchmark/#predicting-category","title":"Predicting Category","text":"<p>First, we train a model to predict the \"F/M\" category. We test out two policies for selecting  excerpts:</p> <ol> <li>Choose excerpts randomly from the entire work.</li> <li>Use FastExcerpt with the default configuration after fitting on the train set.</li> </ol> <p>To evaluate the quality of the chosen snippets, we train another model to predict the tag using only the chosen excerpts as the input and report the area under the ROC curve. We report the  performance when using the top 1 excerpt, the top 3 excerpts, and so on.</p> # Excerpts Random FastExcerpt 1 0.58 0.64 3 0.64 0.68 5 0.67 0.69 10 0.69 0.70 <p>As the above table shows, assuming we only have the capacity to process 1 excerpt, using  <code>FastExcerpt</code> to select it significantly outperforms random selection. However, as the  number of excerpts increases, the gains decrease.</p>"},{"location":"benchmark/#predicting-rating","title":"Predicting Rating","text":"<p>Next, we run the same evaluation procedure but with the \"Explicit\" tag.</p> # Excerpts Random FastExcerpt 1 0.558 0.749 3 0.593 0.748 5 0.604 0.750 10 0.643 0.746 <p>On this tag, we observe an even larger gap in performance between random excerpts and fastexcerpt than the F/M tag. We hypothesize that this is due the fact that the presence of a F/M relationship  can be inferred from many different parts of the work whereas explicit content might be limited to  a specific paragraph/chapter which FastExcerpt is able to find.</p>"},{"location":"roadmap/","title":"Roadmap","text":"<p>This project is still in an exploratory stage. Here is my current plan:</p>"},{"location":"roadmap/#v01-mvp","title":"v0.1 - MVP","text":"<ul> <li>Remove subword dependencies + model.</li> <li>Consider switching to PyTorch backend.</li> <li>Add support for other supervised losses<ul> <li>multi-class</li> <li>multi-label</li> </ul> </li> <li>Code cleanup :)</li> <li>Finalize API design and prepare a release.</li> </ul>"},{"location":"roadmap/#v02-scalability","title":"v0.2 - Scalability","text":"<ul> <li>[Python] Start optimizing for speed - sliding window optimizations, etc.</li> <li>[Go/Rust] Move hashing vectorizer to Go/Rust?</li> <li>Add speed to the benchmark report</li> </ul>"},{"location":"roadmap/#v03-functionality","title":"v0.3 - Functionality","text":"<ul> <li>Enable iterative refinement</li> <li>Add support for unsupervised losses (?)</li> <li>Publish pre-trained models for general categories (i.e. Humor/Drama) as well     as a large unsupervised model trained on our entire dataset.</li> <li>Add a basic cli<ul> <li>fastexcerpt train --dataset ao3.jsonl.bz2 --model model.bin</li> <li>fastexcerpt excerpts --model model.bin --path_to_file example.txt</li> </ul> </li> </ul>"}]}